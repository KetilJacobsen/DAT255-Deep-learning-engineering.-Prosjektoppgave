{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyNk2jSk7h+c1+eDAZ8AXcbf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KetilJacobsen/DAT255-Deep-learning-engineering.-Prosjektoppgave/blob/main/YOLOv11_Small_Version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# In this notebook we will train the YOLOv11 Version small."
      ],
      "metadata": {
        "id": "CJoHJWU5BX8E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "@software{yolo11_ultralytics, \\\n",
        "  author = {Glenn Jocher and Jing Qiu}, \\\n",
        "  title = {Ultralytics YOLO11}, \\\n",
        "  version = {11.0.0}, \\\n",
        "  year = {2024}, \\\n",
        "  url = {https://github.com/ultralytics/ultralytics}, \\\n",
        "  orcid = {0000-0001-5950-6979, 0000-0002-7603-6750, 0000-0003-3783-7069},\\\n",
        "  license = {AGPL-3.0}\n",
        "}"
      ],
      "metadata": {
        "id": "u_oqy8qI5tF0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Install Required Libraries\n",
        "We begin by installing the necessary Python libraries for this project:\n",
        "\n",
        "- **Ultralytics**: Provides the YOLOv11 framework for training, evaluating, and deploying object detection models.\n",
        "- **Roboflow** *(optional)*: Used to easily download and manage datasets hosted on [Roboflow](https://roboflow.com/).\n",
        "- **OpenCV & Matplotlib**: Used later in the notebook to draw ground truth boxes and display side-by-side comparisons between model predictions and actual labels.\n",
        "\n",
        "\n",
        "\n",
        "These libraries are regularly updated, so we include `--upgrade` to ensure the latest features and bug fixes are used."
      ],
      "metadata": {
        "id": "-HZHKPSwQPpm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OurYB_phPgO-",
        "outputId": "88ce13d6-d165-4150-a077-cc772221d3a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.107-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: numpy<=2.1.1,>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.14.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.13.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Downloading ultralytics-8.3.107-py3-none-any.whl (974 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m974.5/974.5 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ultralytics-8.3.107 ultralytics-thop-2.0.14\n",
            "Collecting roboflow\n",
            "  Downloading roboflow-1.1.61-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from roboflow) (2025.1.31)\n",
            "Collecting idna==3.7 (from roboflow)\n",
            "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.11/dist-packages (from roboflow) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.4.8)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from roboflow) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.0.2)\n",
            "Collecting opencv-python-headless==4.10.0.84 (from roboflow)\n",
            "  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from roboflow) (11.1.0)\n",
            "Collecting pillow-heif>=0.18.0 (from roboflow)\n",
            "  Downloading pillow_heif-0.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.8.2)\n",
            "Collecting python-dotenv (from roboflow)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.3.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from roboflow) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (6.0.2)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.0.0)\n",
            "Collecting filetype (from roboflow)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (1.3.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (4.57.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (3.2.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->roboflow) (3.4.1)\n",
            "Downloading roboflow-1.1.61-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.2/85.2 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow_heif-0.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m97.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: filetype, python-dotenv, pillow-heif, opencv-python-headless, idna, roboflow\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.11.0.86\n",
            "    Uninstalling opencv-python-headless-4.11.0.86:\n",
            "      Successfully uninstalled opencv-python-headless-4.11.0.86\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "Successfully installed filetype-1.2.0 idna-3.7 opencv-python-headless-4.10.0.84 pillow-heif-0.22.0 python-dotenv-1.1.0 roboflow-1.1.61\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (2.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics --upgrade\n",
        "!pip install roboflow\n",
        "!pip install opencv-python matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Clone the Ultralytics Repository\n",
        "\n",
        "We clone the official [Ultralytics GitHub repository](https://github.com/ultralytics/ultralytics) to access the YOLOv11 model configuration files directly.\n",
        "\n",
        "This allows us to:\n",
        "- Modify the model architecture (e.g., `yolo11.yaml`)\n",
        "- Train a custom version of YOLOv11\n",
        "\n",
        "Note: The `ultralytics` package is installed via pip and used for training, inference, and evaluation.  \n",
        "We clone the GitHub repo only to edit the model architecture files."
      ],
      "metadata": {
        "id": "Z5VVygRHRFnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ultralytics/ultralytics.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEbkxF16P-f4",
        "outputId": "6d2ff157-2c19-4a45-bcc2-15f78a3cf8cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'ultralytics' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: View and Modify the YOLOv11 Detection Model Configuration\n",
        "\n",
        "The YOLOv11 detection architecture is defined in `yolo11.yaml`.  \n",
        "We view this file to:\n",
        "- Update the number of object classes (`nc`)\n",
        "- Optionally customize the modelâ€™s architecture (e.g., layers, modules, channels)\n",
        "\n",
        "In this case, we change:\n",
        "- `nc: 80` â†’ `nc: 4`  \n",
        "  to match our dataset's four classes: `person`, `aware`, `unaware`, and `partially-aware`.\n",
        "- Add or remove from the backbone to modify the structure.\n",
        "\n",
        "Below, we display the contents of the file before editing:"
      ],
      "metadata": {
        "id": "5SfK4GgGSBnY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Used to view the structure\n",
        "!cat ultralytics/ultralytics/cfg/models/11/yolo11.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsBIqSl0RWs5",
        "outputId": "68ee1243-f1b5-4fd6-bfdf-002c0773bb55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Ultralytics ðŸš€ AGPL-3.0 License - https://ultralytics.com/license\n",
            "\n",
            "# Ultralytics YOLO11 object detection model with P3/8 - P5/32 outputs\n",
            "# Model docs: https://docs.ultralytics.com/models/yolo11\n",
            "# Task docs: https://docs.ultralytics.com/tasks/detect\n",
            "\n",
            "# Parameters\n",
            "nc: 4 # number of classes\n",
            "scales: # model compound scaling constants, i.e. 'model=yolo11n.yaml' will call yolo11.yaml with scale 'n'\n",
            "  # [depth, width, max_channels]\n",
            "  n: [0.50, 0.25, 1024] # summary: 181 layers, 2624080 parameters, 2624064 gradients, 6.6 GFLOPs\n",
            "  s: [0.50, 0.50, 1024] # summary: 181 layers, 9458752 parameters, 9458736 gradients, 21.7 GFLOPs\n",
            "  m: [0.50, 1.00, 512] # summary: 231 layers, 20114688 parameters, 20114672 gradients, 68.5 GFLOPs\n",
            "  l: [1.00, 1.00, 512] # summary: 357 layers, 25372160 parameters, 25372144 gradients, 87.6 GFLOPs\n",
            "  x: [1.00, 1.50, 512] # summary: 357 layers, 56966176 parameters, 56966160 gradients, 196.0 GFLOPs\n",
            "\n",
            "# YOLO11n backbone\n",
            "backbone:\n",
            "  # [from, repeats, module, args]\n",
            "  - [-1, 1, Conv, [64, 3, 2]] # 0-P1/2\n",
            "  - [-1, 1, Conv, [128, 3, 2]] # 1-P2/4\n",
            "  - [-1, 2, C3k2, [256, False, 0.25]]\n",
            "  - [-1, 1, Conv, [256, 3, 2]] # 3-P3/8\n",
            "  - [-1, 2, C3k2, [512, False, 0.25]]\n",
            "  - [-1, 1, Conv, [512, 3, 2]] # 5-P4/16\n",
            "  - [-1, 2, C3k2, [512, True]]\n",
            "  - [-1, 1, Conv, [1024, 3, 2]] # 7-P5/32\n",
            "  - [-1, 2, C3k2, [1024, True]]\n",
            "  - [-1, 1, SPPF, [1024, 5]] # 9\n",
            "  - [-1, 2, C2PSA, [1024]] # 10\n",
            "\n",
            "# YOLO11n head\n",
            "head:\n",
            "  - [-1, 1, nn.Upsample, [None, 2, \"nearest\"]]\n",
            "  - [[-1, 6], 1, Concat, [1]] # cat backbone P4\n",
            "  - [-1, 2, C3k2, [512, False]] # 13\n",
            "\n",
            "  - [-1, 1, nn.Upsample, [None, 2, \"nearest\"]]\n",
            "  - [[-1, 4], 1, Concat, [1]] # cat backbone P3\n",
            "  - [-1, 2, C3k2, [256, False]] # 16 (P3/8-small)\n",
            "\n",
            "  - [-1, 1, Conv, [256, 3, 2]]\n",
            "  - [[-1, 13], 1, Concat, [1]] # cat head P4\n",
            "  - [-1, 2, C3k2, [512, False]] # 19 (P4/16-medium)\n",
            "\n",
            "  - [-1, 1, Conv, [512, 3, 2]]\n",
            "  - [[-1, 10], 1, Concat, [1]] # cat head P5\n",
            "  - [-1, 2, C3k2, [1024, True]] # 22 (P5/32-large)\n",
            "\n",
            "  - [[16, 19, 22], 1, Detect, [nc]] # Detect(P3, P4, P5)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To enhance the model's capacity for extracting context-rich and subtle features (such as head orientation and posture), we added a 3Ã—3 convolutional layer followed by an additional C3k2 block with 1024 channels before the final SPPF and C2PSA layers. The Conv layer helps reorganize the feature maps for better gradient flow, while the C3k2 block deepens the network, allowing it to capture more abstract representations. This structure aims to improve detection performance for our 4-class awareness classification task."
      ],
      "metadata": {
        "id": "z-YdNQcgnuqU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to change the yaml file and what was changed\n",
        "\n",
        "Add the: \"%%writefile ultralytics/ultralytics/cfg/models/11/yolo11.yaml\" \\\n",
        "This is used to modify and overvrite the structure below.\n",
        "First we change the number of classes (nc) to 4 and then we added block 9 and 10 below"
      ],
      "metadata": {
        "id": "2MlMpowhoHhm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ultralytics/ultralytics/cfg/models/11/yolo11s.yaml\n",
        "# Ultralytics ðŸš€ AGPL-3.0 License - https://ultralytics.com/license\n",
        "\n",
        "# Ultralytics YOLO11 object detection model with P3/8 - P5/32 outputs\n",
        "# Model docs: https://docs.ultralytics.com/models/yolo11\n",
        "# Task docs: https://docs.ultralytics.com/tasks/detect\n",
        "\n",
        "# Parameters\n",
        "nc: 4 # number of classes\n",
        "scales: # model compound scaling constants, i.e. 'model=yolo11n.yaml' will call yolo11.yaml with scale 'n'\n",
        "  # [depth, width, max_channels]\n",
        "  s: [0.50, 0.50, 1024] # summary: 181 layers, 9458752 parameters, 9458736 gradients, 21.7 GFLOPs\n",
        "\n",
        "\n",
        "# YOLO11n backbone\n",
        "backbone:\n",
        "  # [from, repeats, module, args]\n",
        "  - [-1, 1, Conv, [64, 3, 2]] # 0-P1/2\n",
        "  - [-1, 1, Conv, [128, 3, 2]] # 1-P2/4\n",
        "  - [-1, 2, C3k2, [256, False, 0.25]]\n",
        "  - [-1, 1, Conv, [256, 3, 2]] # 3-P3/8\n",
        "  - [-1, 2, C3k2, [512, False, 0.25]]\n",
        "  - [-1, 1, Conv, [512, 3, 2]] # 5-P4/16\n",
        "  - [-1, 2, C3k2, [512, True]]\n",
        "  - [-1, 1, Conv, [1024, 3, 2]] # 7-P5/32\n",
        "  - [-1, 2, C3k2, [1024, True]]\n",
        "  - [-1, 1, SPPF, [1024, 5]] # 9\n",
        "  - [-1, 2, C2PSA, [1024]] # 10\n",
        "\n",
        "# YOLO11n head\n",
        "head:\n",
        "  - [-1, 1, nn.Upsample, [None, 2, \"nearest\"]]\n",
        "  - [[-1, 6], 1, Concat, [1]] # cat backbone P4\n",
        "  - [-1, 2, C3k2, [512, False]] # 13\n",
        "\n",
        "  - [-1, 1, nn.Upsample, [None, 2, \"nearest\"]]\n",
        "  - [[-1, 4], 1, Concat, [1]] # cat backbone P3\n",
        "  - [-1, 2, C3k2, [256, False]] # 16 (P3/8-small)\n",
        "\n",
        "  - [-1, 1, Conv, [256, 3, 2]]\n",
        "  - [[-1, 13], 1, Concat, [1]] # cat head P4\n",
        "  - [-1, 2, C3k2, [512, False]] # 19 (P4/16-medium)\n",
        "\n",
        "  - [-1, 1, Conv, [512, 3, 2]]\n",
        "  - [[-1, 10], 1, Concat, [1]] # cat head P5\n",
        "  - [-1, 2, C3k2, [1024, True]] # 22 (P5/32-large)\n",
        "\n",
        "  - [[16, 19, 22], 1, Detect, [nc]] # Detect(P3, P4, P5)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3T0yhAHTIZl",
        "outputId": "86177af4-4e9c-488f-fa08-9d28c210b7f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing ultralytics/ultralytics/cfg/models/11/yolo11s.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Used to view the structure after chainging the structure\n",
        "!cat ultralytics/ultralytics/cfg/models/11/yolo11s.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzzCDzFRBMdz",
        "outputId": "fddac6b8-5a8a-4966-c3a1-5ad7adf1ee5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Ultralytics ðŸš€ AGPL-3.0 License - https://ultralytics.com/license\n",
            "\n",
            "# Ultralytics YOLO11 object detection model with P3/8 - P5/32 outputs\n",
            "# Model docs: https://docs.ultralytics.com/models/yolo11\n",
            "# Task docs: https://docs.ultralytics.com/tasks/detect\n",
            "\n",
            "# Parameters\n",
            "nc: 4 # number of classes\n",
            "depth_multiple: 0.50\n",
            "width_multiple: 0.50\n",
            "\n",
            "scales: # model compound scaling constants, i.e. 'model=yolo11n.yaml' will call yolo11.yaml with scale 'n'\n",
            "  # [depth, width, max_channels]\n",
            "  s: [0.50, 0.50, 1024] # summary: 181 layers, 9458752 parameters, 9458736 gradients, 21.7 GFLOPs\n",
            "\n",
            "\n",
            "# YOLO11n backbone\n",
            "backbone:\n",
            "  # [from, repeats, module, args]\n",
            "  - [-1, 1, Conv, [64, 3, 2]] # 0-P1/2\n",
            "  - [-1, 1, Conv, [128, 3, 2]] # 1-P2/4\n",
            "  - [-1, 2, C3k2, [256, False, 0.25]]\n",
            "  - [-1, 1, Conv, [256, 3, 2]] # 3-P3/8\n",
            "  - [-1, 2, C3k2, [512, False, 0.25]]\n",
            "  - [-1, 1, Conv, [512, 3, 2]] # 5-P4/16\n",
            "  - [-1, 2, C3k2, [512, True]]\n",
            "  - [-1, 1, Conv, [1024, 3, 2]] # 7-P5/32\n",
            "  - [-1, 2, C3k2, [1024, True]]\n",
            "  - [-1, 1, SPPF, [1024, 5]] # 9\n",
            "  - [-1, 2, C2PSA, [1024]] # 10\n",
            "\n",
            "# YOLO11n head\n",
            "head:\n",
            "  - [-1, 1, nn.Upsample, [None, 2, \"nearest\"]]\n",
            "  - [[-1, 6], 1, Concat, [1]] # cat backbone P4\n",
            "  - [-1, 2, C3k2, [512, False]] # 13\n",
            "\n",
            "  - [-1, 1, nn.Upsample, [None, 2, \"nearest\"]]\n",
            "  - [[-1, 4], 1, Concat, [1]] # cat backbone P3\n",
            "  - [-1, 2, C3k2, [256, False]] # 16 (P3/8-small)\n",
            "\n",
            "  - [-1, 1, Conv, [256, 3, 2]]\n",
            "  - [[-1, 13], 1, Concat, [1]] # cat head P4\n",
            "  - [-1, 2, C3k2, [512, False]] # 19 (P4/16-medium)\n",
            "\n",
            "  - [-1, 1, Conv, [512, 3, 2]]\n",
            "  - [[-1, 10], 1, Concat, [1]] # cat head P5\n",
            "  - [-1, 2, C3k2, [1024, True]] # 22 (P5/32-large)\n",
            "\n",
            "  - [[16, 19, 22], 1, Detect, [nc]] # Detect(P3, P4, P5)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Download Our Custom Dataset from Roboflow\n",
        "\n",
        "We use the Roboflow Python SDK to download our own annotated dataset,  \n",
        "`human_awareness_face` (version 39), from the **HVL Robotics workspace**.\n",
        "\n",
        "This dataset was created and labeled by our project group to support  \n",
        "awareness classification of humans in agriculture.\n",
        "\n",
        "We download it in **YOLOv11 format**, ensuring compatibility with the custom YOLOv11 model configuration  \n",
        "we are using for training.\n",
        "\n",
        "The download includes:\n",
        "- `train`, `valid`, and `test` image sets\n",
        "- A `data.yaml` file with class names and paths\n"
      ],
      "metadata": {
        "id": "hSusuqB0UELY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"CsyOoX5KDG78hhUgnDyj\")\n",
        "project = rf.workspace(\"hvl-robotics\").project(\"human_awareness_face\")\n",
        "version = project.version(39)\n",
        "dataset = version.download(\"yolov11\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGpbiRHnT2Se",
        "outputId": "c7a43fe0-59ee-4c5c-d572-4d4afd412672"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Train the Custom YOLOv11 Detection Model size Nano\n",
        "\n",
        "We now train our custom YOLOv11 object detection model using the modified architecture  \n",
        "defined in `yolo11.yaml`, and the dataset we downloaded from Roboflow.\n",
        "\n",
        "Key training parameters:\n",
        "- `model='n',  # nano scale`\n",
        "- `data=\"/content/human_awareness_face-39/data.yaml`\n",
        "- `epochs=50`\n",
        "- `imgsz=640`\n",
        "- `batch=16`\n",
        "- `name=\"yolo11n_custom_aug_es`,  # custom name for logs \\\n",
        "\n",
        "# Data augmentations\n",
        "- `shear=10`\n",
        "- `hsv_h=0.015`\n",
        "- `hsv_s=0.7`\n",
        "- `hsv_v=0.4`\n",
        "\n",
        "# Early stopping\n",
        "- `patience=10,results`015`\n",
        "- `hsv_s=0.7`\n",
        "- `hsv_v=0.4`\n",
        "\n",
        "# Early stopping\n",
        "- `patience=10,results`\n",
        "\n",
        "The model will be trained to detect and classify the following 4 classes:\n",
        "- `person`\n",
        "- `aware`\n",
        "- `unaware`\n",
        "- `partially-aware`"
      ],
      "metadata": {
        "id": "ncCTOJ-eUk1u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We experimented with different YOLOv11 scale variants by leveraging Ultralytics' compound scaling system. The 'n' (nano) version offers faster training and inference with lower accuracy, while the 's' (small) variant balances performance and resource use. These variants are activated by adjusting the model scale parameter in the training function. \\\n",
        "To avoid overfitting and reduce unnecessary training time, we enabled early stopping with a patience of 10 epochs. This ensures training halts once performance plateaus, preserving the best-performing model based on validation metrics."
      ],
      "metadata": {
        "id": "_uxvlZAJq7IT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# Load the YOLOv11 model\n",
        "model = YOLO(\"ultralytics/ultralytics/cfg/models/11/yolo11s.yaml\")\n",
        "\n",
        "# Train the small version\n",
        "model.train(\n",
        "    model='s',  # small scale\n",
        "    data=\"/content/human_awareness_face-39/data.yaml\",\n",
        "    epochs=50,\n",
        "    imgsz=640,\n",
        "    batch=16,\n",
        "    name=\"yolo11s_aug_es\",  # custom name for logs\n",
        "\n",
        "    # Data augmentations\n",
        "    shear=10,\n",
        "    hsv_h=0.010,\n",
        "    hsv_s=0.25,\n",
        "    hsv_v=0.15,\n",
        "\n",
        "    # Early stopping\n",
        "    patience=10,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7-Sj5M_UyaU",
        "outputId": "326ebd84-4ee0-49a4-f4f7-9a47602df75c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.107 ðŸš€ Python-3.11.12 torch-2.6.0+cu124 CPU (Intel Xeon 2.20GHz)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=s, data=/content/human_awareness_face-39/data.yaml, epochs=50, time=None, patience=10, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=yolo11s_aug_es2, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=10, translate=0.1, scale=0.5, shear=10, perspective=0.001, flipud=0.2, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.2, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/yolo11s_aug_es2\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
            "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  2                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
            "  3                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            "  4                  -1  1    103360  ultralytics.nn.modules.block.C3k2            [128, 256, 1, False, 0.25]    \n",
            "  5                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            "  6                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
            "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
            "  8                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n",
            "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
            " 10                  -1  1    990976  ultralytics.nn.modules.block.C2PSA           [512, 512, 1]                 \n",
            " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 13                  -1  1    443776  ultralytics.nn.modules.block.C3k2            [768, 256, 1, False]          \n",
            " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 16                  -1  1    127680  ultralytics.nn.modules.block.C3k2            [512, 128, 1, False]          \n",
            " 17                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 19                  -1  1    345472  ultralytics.nn.modules.block.C3k2            [384, 256, 1, False]          \n",
            " 20                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 22                  -1  1   1511424  ultralytics.nn.modules.block.C3k2            [768, 512, 1, True]           \n",
            " 23        [16, 19, 22]  1    820956  ultralytics.nn.modules.head.Detect           [4, [128, 256, 512]]          \n",
            "YOLO11s summary: 181 layers, 9,429,340 parameters, 9,429,324 gradients, 21.6 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/yolo11s_aug_es2', view at http://localhost:6006/\n",
            "Freezing layer 'model.23.dfl.conv.weight'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/human_awareness_face-39/train/labels.cache... 541 images, 29 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 541/541 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/human_awareness_face-39/valid/labels.cache... 151 images, 7 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 151/151 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plotting labels to runs/detect/yolo11s_aug_es2/labels.jpg... \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.00125, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n",
            "Image sizes 640 train, 640 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/yolo11s_aug_es2\u001b[0m\n",
            "Starting training for 50 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       1/50         0G      3.798      4.846      4.208         93        640:   6%|â–Œ         | 2/34 [01:42<27:03, 50.74s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Evaluate the Trained YOLOv11 Model\n",
        "\n",
        "After training, we load the best-performing weights (`best.pt`)  \n",
        "from the `runs/detect/yolo11_custom/weights/` directory and evaluate the model on the validation set.\n",
        "\n",
        "This evaluation provides key performance metrics:\n",
        "- **Precision**: How many of the predicted bounding boxes are correct\n",
        "- **Recall**: How many of the actual objects were detected\n",
        "- **mAP@0.5**: Mean Average Precision at IoU 0.5\n",
        "- **mAP@0.5:0.95**: Average over 10 IoU thresholds from 0.5 to 0.95 (stricter and more informative)\n",
        "\n",
        "It also generates visual outputs such as:\n",
        "- Confusion matrix\n",
        "- Precision-recall curves\n",
        "- Overall results summary plots\n"
      ],
      "metadata": {
        "id": "k0YIyyQRU9yZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Total number of model parameters\n",
        "total_params = sum(p.numel() for p in model.model.parameters())\n",
        "print(f\"Number of parameters: {total_params:,}\")\n",
        "model.model.info(verbose=True)"
      ],
      "metadata": {
        "id": "RKOrs3iAUHgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# Load best weights from the nano model\n",
        "model = YOLO(\"runs/detect/yolo11s_aug_es/weights/best.pt\")\n",
        "\n",
        "# Evaluate on the validation set (same one used during training)\n",
        "metrics = model.val()\n"
      ],
      "metadata": {
        "id": "43BA8GrpU9ds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training dataset consisted of 541 original images, annotated across four classes: 'aware', 'partially-aware', 'person', and 'unaware'. To improve generalization and prevent overfitting, extensive on-the-fly data augmentation was applied, including random scaling, flipping, rotation, HSV color shifts, mosaic, and mixup. As a result, the model effectively saw thousands of unique image variations across 50 training epochs, even though the core dataset remained the same."
      ],
      "metadata": {
        "id": "ILdNYpsg0ngu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7: Explore Evaluation Output Files\n",
        "\n",
        "After evaluating the trained model, Ultralytics automatically generates a `runs/detect/val/` folder  \n",
        "containing all visual and numerical outputs related to model performance.\n",
        "\n",
        "This includes:\n",
        "- `confusion_matrix.png`: Class-level confusion matrix\n",
        "- `PR_curve.png`: Precision-recall curve for each class\n",
        "- `results.png`: Combined loss and metric plots\n",
        "- `labels.jpg`: Annotated image overview from the validation set\n",
        "\n",
        "We list the contents of this folder below to confirm that evaluation outputs were generated.\n"
      ],
      "metadata": {
        "id": "-FrWRmDKVRFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls runs/detect/val/"
      ],
      "metadata": {
        "id": "MunIDUVGVRc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 8: Visualize Evaluation Results\n",
        "\n",
        "To better understand the performance of the model, we display key visual outputs from the evaluation step:\n",
        "\n",
        "- **Confusion Matrix**: Shows how well the model distinguishes between the four classes.\n",
        "- **Results Summary**: Includes training loss curves, precision, recall, and mAP progression over epochs.\n",
        "\n",
        "These visuals help diagnose performance bottlenecks (e.g., misclassifications between similar classes like `aware` vs `partially-aware`) and guide further improvements.\n"
      ],
      "metadata": {
        "id": "nsLNu43oVcW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "# Show confusion matrix\n",
        "display(Image(filename='runs/detect/val/confusion_matrix.png'))\n",
        "\n",
        "# Show results summary (make sure this path matches the latest run)\n",
        "display(Image(filename='/content/runs/detect/yolo11s_aug_es/results.png'))\n"
      ],
      "metadata": {
        "id": "PZPtIZBUVqt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 9: Display Key Evaluation Metrics\n",
        "\n",
        "To summarize the performance of the trained model, we print the core evaluation metrics:\n",
        "\n",
        "- **Precision**: The percentage of predicted bounding boxes that are correct\n",
        "- **Recall**: The percentage of actual objects that were detected\n",
        "- **mAP@0.5**: Mean Average Precision at 50% IoU (standard object detection score)\n",
        "- **mAP@0.5:0.95**: Stricter mean Average Precision averaged across 10 IoU thresholds (from 0.5 to 0.95)\n",
        "\n",
        "These values provide a quick, numerical snapshot of how well the model performs on the validation set.\n"
      ],
      "metadata": {
        "id": "9QOjjWNzVzVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this after: metrics = model.val()\n",
        "\n",
        "print(f\"Precision:      {metrics.box.mp:.3f}\")\n",
        "print(f\"Recall:         {metrics.box.mr:.3f}\")\n",
        "print(f\"mAP@0.5:        {metrics.box.map50:.3f}\")\n",
        "print(f\"mAP@0.5:0.95:   {metrics.box.map:.3f}\")\n"
      ],
      "metadata": {
        "id": "_rBBXQfsV7D-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from ultralytics import YOLO\n",
        "import glob\n",
        "\n",
        "# === CONFIGURATION ===\n",
        "val_img_dir = \"/content/human_awareness_face-39/valid/images\"\n",
        "val_lbl_dir = \"/content/human_awareness_face-39/valid/labels\"\n",
        "model_path = \"runs/detect/yolo11s_aug_es/weights/best.pt\"\n",
        "num_images_to_show = 5  # Adjust this number\n",
        "\n",
        "# === LOAD MODEL ===\n",
        "model = YOLO(model_path)\n",
        "class_names = model.names  # Automatically uses correct class order\n",
        "\n",
        "# === COLOR MAP MATCHING CLASS ORDER IN YOUR YAML ===\n",
        "# ['aware', 'partially-aware', 'person', 'unaware']\n",
        "color_map = {\n",
        "    0: (0, 255, 0),       # aware - green\n",
        "    1: (255, 165, 0),     # partially-aware - orange\n",
        "    2: (0, 0, 255),       # person - blue\n",
        "    3: (255, 0, 0),       # unaware - red\n",
        "}\n",
        "\n",
        "# === GET IMAGE LIST ===\n",
        "image_paths = glob.glob(os.path.join(val_img_dir, \"*.jpg\"))\n",
        "\n",
        "# === LOOP THROUGH IMAGES ===\n",
        "for idx, img_path in enumerate(image_paths[:num_images_to_show]):\n",
        "    img_name = os.path.basename(img_path)\n",
        "    label_path = os.path.join(val_lbl_dir, img_name.replace(\".jpg\", \".txt\"))\n",
        "\n",
        "    # Load image\n",
        "    image = cv2.imread(img_path)\n",
        "    if image is None or not os.path.exists(label_path):\n",
        "        print(f\"Skipping {img_name} (missing image or label)\")\n",
        "        continue\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    h, w = image_rgb.shape[:2]\n",
        "\n",
        "    # === DRAW GROUND TRUTH ===\n",
        "    img_gt = image_rgb.copy()\n",
        "    with open(label_path, 'r') as f:\n",
        "        for line in f:\n",
        "            cls_id, x, y, bw, bh = map(float, line.strip().split())\n",
        "            x1 = int((x - bw/2) * w)\n",
        "            y1 = int((y - bh/2) * h)\n",
        "            x2 = int((x + bw/2) * w)\n",
        "            y2 = int((y + bh/2) * h)\n",
        "            cls_id = int(cls_id)\n",
        "            cls_name = class_names[cls_id]\n",
        "            color = color_map.get(cls_id, (0, 255, 255))  # fallback to yellow\n",
        "\n",
        "            # Draw box\n",
        "            cv2.rectangle(img_gt, (x1, y1), (x2, y2), color, 2)\n",
        "\n",
        "            # Draw label with background\n",
        "            label = f\"{cls_name}\"\n",
        "            (text_w, text_h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.4, 1)\n",
        "            cv2.rectangle(img_gt, (x1, y1 - text_h - 6), (x1 + text_w + 4, y1), color, -1)\n",
        "            cv2.putText(img_gt, label, (x1 + 2, y1 - 4),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 0, 0), 1)\n",
        "\n",
        "    # === PREDICTIONS ===\n",
        "    results = model(img_path)[0]\n",
        "    img_pred = image_rgb.copy()\n",
        "    for box in results.boxes:\n",
        "        x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())\n",
        "        cls_id = int(box.cls[0])\n",
        "        conf = box.conf[0].item()\n",
        "        cls_name = class_names[cls_id]\n",
        "        color = color_map.get(cls_id, (0, 255, 255))\n",
        "\n",
        "        # Draw box\n",
        "        cv2.rectangle(img_pred, (x1, y1), (x2, y2), color, 2)\n",
        "\n",
        "        # Draw label with background\n",
        "        label = f\"{cls_name} {conf:.2f}\"\n",
        "        (text_w, text_h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.4, 1)\n",
        "        cv2.rectangle(img_pred, (x1, y1 - text_h - 6), (x1 + text_w + 4, y1), color, -1)\n",
        "        cv2.putText(img_pred, label, (x1 + 2, y1 - 4),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 0, 0), 1)\n",
        "\n",
        "    # === DISPLAY ===\n",
        "    plt.figure(figsize=(14, 7))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(img_gt)\n",
        "    plt.title(f\"Ground Truth: {img_name}\", fontsize=14)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(img_pred)\n",
        "    plt.title(f\"Prediction: {img_name}\", fontsize=14)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "73KnC3Q0rrKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZpP74rVyXRmZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}